
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Improving Visual Generalization in Model-Based Reinforcement Learning</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://mngupark.github.io/vigmo"/>
    <meta property="og:title" content="Improving Visual Generalization in Model-Based Reinforcement Learning" />
    <meta property="og:description" content="Combining recipes from the model-free visual RL literature with a model-based RL backbone achieves superior sample efficiency and visual generalization across diverse visual RL environments." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Improving Visual Generalization in Model-Based Reinforcement Learning" />
    <meta name="twitter:description" content="Combining recipes from the model-free visual RL literature with a model-based RL backbone achieves superior sample efficiency and visual generalization across diverse visual RL environments." />
    <meta name="twitter:image" content="https://mngupark.github.io/vigmo/img/overview.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üïπÔ∏è</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/app.css">

    <link rel="stylesheet" href="../assets/css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script type="text/javascript" src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js"></script>

    <script src="../assets/js/app.js"></script>
    <script src="../assets/js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center" style="font-family: 'Times New Roman', Times, serif">
                Improving Visual Generalization in Model-Based Reinforcement Learning</br> 
                <small>
                Under review, 2025
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mngupark.github.io/">
                          Mingyu Park
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/donghwanleehome/about-pi">
                            Donghwan Lee
                        </a>
                    </li>
                    </br>KAIST (Korea Advanced Institute of Science and Technology)
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="../data/icml_vigmo.pdf">
                        <image src="../assets/img/icon/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/mngupark">
                        <image src="../assets/img/icon/github.png" height="60px">
                            <h4><strong>Code (TBA)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h3 class="text-center" style="font-size: 32px; font-weight: bold;">
                    Abstract
                </h3>
                <p class="text-justify lead">
                    Learning a generalizable reinforcement learning (RL) agent to the unseen visual image in a zero-shot manner enables further deployments of deep RL into the real world.
                    The field has witnessed significant progress in the prior literature by leveraging data augmentation and auxiliary representation learning techniques.
                    However, simultaneously achieving superior sample efficiency and generalization ability still remains challenging for visual RL agents.
                    In this work, we devise <b>Vi</b>sual <b>G</b>eneralization in <b>MO</b>del-Based RL (<b>ViGMO</b>), a novel model-based RL method to encourage visual generalization with superior sample efficiency by blending a popular model-based RL architecture with groundbreaking recipes from the prior literature in model-free RL.
                    Our key idea is to constrain the model to exhibit a consistent prediction ability regardless of visual perturbations during training.
                    We provide extensive empirical results on the sample efficiency and generalization ability of visual RL agents in diverse environments and tasks.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <h3 class="text-center" style="font-size: 32px; font-weight: bold;">
                    Overview
                </h3>
				<table style="width: 100%; border-collapse: collapse; margin-bottom:10px">
				  <tr>
				    <td style="text-align: center;">
		                <image src="img/overview.jpg" width="70%">
					</td>
				  </tr>
				</table>
                <p class="text-justify lead">
                    We propose <b>ViGMO</b>; a model-based RL for visual generalization with superior sample efficiency by employing recipes from model-free RL:
                    (1) applying weak and strong image augmentations,
                    (2) predicting consistent representations from mixed horizontal representations,
                    (3) regularizing visual encoder with weak and strong augmentations.
                </p>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-12 text-center">
                <h3 class="text-center" style="font-size: 32px; font-weight: bold;">
                    Motivation
                </h3>
				<table style="width: 100%; border-collapse: collapse; margin-bottom:10px">
				  <tr>
				    <td style="text-align: center;">
		                <image src="img/motivation.png" width="70%">
					</td>
				  </tr>
				</table>
                <ul class="text-justify lead" style="font-weight: 400;">
                    <li>Typical visual model-based RL generates synthetic latent samples by rolling a learned transition model from latent representations, which enables sample-efficient value learning.</li>
                    <li>However, when the image is perturbed with distracting factors, the encoder may extract representations distinct from the observed latent distribution $Z$.</li>
                    <li>Latent transition model would be conditioned <font color="red">out-of-distributional latent representations</font>, leading to poor performance at test time.</li>
                </ul>
                <p class="text-justify lead">
                    If the transition model predicts consistent representation regardless of perturbations, model-based RL exhibits superior sample efficiency and visual generalization over model-free RL!
                </p>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-12 text-center">
                <h3 class="text-center" style="font-size: 32px; font-weight: bold;">
                    Experimental Results
                </h3>
				<table style="width: 70%; border-collapse: collapse; margin: 0px auto; margin-bottom: 10px;">
                    <tr>
                        <td colspan="2" style="text-align: center;">
                            <image src="img/envs.png" display="block" width="100%" height="100%" object-fit="fill">
                        </td>
                    </tr>
                    <tr>
				        <td colspan="2" style="text-align: center;">
		                    <p>DM Control and Robosuite <b>training</b> tasks</p>
    					</td>
	    			</tr>
                    <tr>
                        <td style="text-align: center;">
                            <image src="img/dmc_eval_types.png" display="block" width="100%" object-fit="fill">
                        </td>
                        <td style="text-align: center;">
                            <image src="img/robosuite_eval_types.png" display="block" width="100%" object-fit="fill">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
		                    <p>Example <b>evaluation</b> tasks on DM Control</p>
    					</td>
				        <td style="text-align: center;">
		                    <p>Example <b>evaluation</b> tasks on Robosuite</p>
    					</td>
	    			</tr>
				</table>
                <p class="text-justify lead">
                    Inspired by <a class="lead" href="https://github.com/gemcollector/RL-ViGen.git">RL-ViGen</a>, we consider diverse visual generalization tasks during evaluation, including <b>15</b> tasks for DM Control and <b>4</b> tasks for Robosuite.
                </p>
                <table style="width: 70%; border-collapse: collapse; margin: 0px auto; margin-bottom: 10px;">
                    <tr>
                        <td style="text-align: center;">
                            <image src="img/dmc_result.png" display="block" width="100%" height="100%" object-fit="fill">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
		                    <p><b>DM Control</b> training & evaluation results</p>
    					</td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
                            <image src="img/robosuite_result.png" display="block" width="100%" height="100%" object-fit="fill">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
		                    <p><b>Robosuite</b> training & evaluation results</p>
    					</td>
	    			</tr>
				</table>
                <p class="text-justify lead">
                    Our method demonstrate strong performance on diverse visual RL tasks with superior sample efficiency and generalization performance.
                </p>
                <table style="width: 70%; border-collapse: collapse; margin: 0px auto; margin-bottom: 10px;">
                    <tr>
                        <td>
                            <image src="img/eval_over_envs.png" width="100%">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
		                    <p>Full <b>evaluation</b> results</p>
    					</td>
	    			</tr>
                    <tr>
                        <td>
                            <image src="img/eval_during_train_over_envs.png" width="100%">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
		                    <p>Full <b>training</b> results</p>
    					</td>
	    			</tr>
				</table>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-12 text-center">
                <h3 class="text-center" style="font-size: 32px; font-weight: bold;">
                    Embedding Visualization
                </h3>
                <table style="width: 100%; border-collapse: collapse; margin: 0px auto; margin-bottom: 10px;">
                    <tr>
                        <td>
                            <iframe width="95%" height="400px" src="https://youtube.com/embed/YLoYlM-0Fag" frameborder="0" allowfullscreen></iframe>
                        </td>
                        <td>
                            <iframe width="95%" height="400px" src="https://youtube.com/embed/7OWTuqN-27k" frameborder="0" allowfullscreen></iframe>
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
                            <p><font color="red"><b>TD-MPC2</b></font> on cheetah-run</p>
                        </td>
                        <td style="text-align: center;">
                            <p><font color="blue"><b>ViGMO</b></font> on cheetah-run</p>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <iframe width="95%" height="400px" src="https://youtube.com/embed/0klSx4Qgbzg" frameborder="0" allowfullscreen></iframe>
                        </td>
                        <td>
                            <iframe width="95%" height="400px" src="https://youtube.com/embed/sEh4Vhg5n40" frameborder="0" allowfullscreen></iframe>
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
                            <p><font color="red"><b>TD-MPC2</b></font> on cartpole-swingup</p>
                        </td>
                        <td style="text-align: center;">
                            <p><font color="blue"><b>ViGMO</b></font> on cartpole-swingup</p>
                        </td>
                    </tr>
                </table>
                <p class="text-justify lead">
                    To validate our motivation, we visualize the latent representation predictions of <font color="red">TD-MPC2</font>, our backbone model-based RL, and <font color="blue">ViGMO</font> over the horizon.
                    While <font color="red">TD-MPC2</font> struggles to predict the consistent representations over different perturbations, <font color="blue">ViGMO</font> demonstrates a consistent prediciton ability over the horizon regardless of perturbations (representations from perturbed images (&times; and &diams;) track the representations extracted from original images (&star;)).
                </p>
            </div>
        </div>
<br>
</body>
</html>
